# Awesome-spatial-visual-reasoning-MLLMs

Repository for awesome spatial/visual reasoning MLLMs.  (focus more on embodied applications)

![Research Topics in Spatial-Visual Reasoning](wordcloud.png)

## Image

1. [arxiv 2505] One RL to See Them All: Visual Triple Unified Reinforcement Learning [[Code](https://github.com/MiniMax-AI/One-RL-to-See-Them-All)] [[Dataset](https://huggingface.co/datasets/One-RL-to-See-Them-All/Orsta-Data-47k)] 
2. [arxiv 2505] DeepEyes: Incentivizing ‚ÄúThinking with Images‚Äù via Reinforcement Learning [[Code](https://github.com/Visual-Agent/DeepEyes)] [[Dataset](https://huggingface.co/datasets/ChenShawn/DeepEyes-Datasets-47k)]
3. [arxiv 2505] R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO [[Code](https://github.com/HJYao00/R1-ShareVL)]
4. [arxiv 2505]  Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models [[Code](https://github.com/kokolerk/TON)] [[Datasets](https://huggingface.co/collections/kolerk/ton-682ad9038395c21e228a645b)]  
5. [arxiv2505] Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning [[Datasets](https://huggingface.co/collections/TIGER-Lab/pixel-reasoner-682fe96ea946d10dda60d24e)] [[Code](https://github.com/TIGER-AI-Lab/Pixel-Reasoner)]
6. [arxiv 2505] VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction [[Code](https://github.com/VITA-Group/VLM-3R)]
7. [arxiv 2025] G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning [[Code](https://github.com/chenllliang/G1)]
8. [arxiv 2505] SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward  [[Code](https://github.com/kxfan2002/SophiaVL-R1)] [[Dataset](https://huggingface.co/datasets/bunny127/SophiaVL-R1-Thinking-156k)]
9. [arxiv 2505] VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning [[Code](https://github.com/dvlab-research/VisionReasoner)] [[Dataset](https://huggingface.co/datasets/Ricky06662/VisionReasoner_multi_object_7k_840)]
10. [arxiv 2505] GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning [[Code](https://github.com/yueliu1999/GuardReasoner-VL)]
11. [arxiv 2505] Multi-SpatialMLLM Multi-Frame Spatial Understanding with Multi-Modal Large Language Models  [[Code](https://github.com/facebookresearch/Multi-SpatialMLLM)] 
12. [arxiv 2505] GRIT: Teaching MLLMs to Think with Images [[Code](https://github.com/eric-ai-lab/GRIT)] 
13. [arxiv 2505] Visual Agentic Reinforcement Fine-Tuning [[Code](https://github.com/Liuziyu77/Visual-RFT)] [[Dataset](https://huggingface.co/collections/laolao77/visual-arft-682c601d0e35ac6470adfe9f)]
14. [arxiv 2505] OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning  [[Code](https://github.com/zhaochen0110/OpenThinkIMG)] [[Dataset](https://huggingface.co/collections/Warrieryes/openthinkimg-68244a63e97a24d9b7ffcde9)]
15. [arxiv 2504] Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning [[Code](https://github.com/SkyworkAI/Skywork-R1V)]
16. [arxiv 2504] Perception-R1: Pioneering Perception Policy with Reinforcement Learning  [[Code](https://github.com/linkangheng/PR1)] [[Dataset](https://huggingface.co/collections/Kangheng/perception-r1-67f6b14f89d307a0ece985af)]
17. [arxiv 2504] SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement  [[Code](https://github.com/si0wang/ThinkLite-VL)] [[Datasets](https://huggingface.co/collections/russwang/thinklite-vl-67f88c6493f8a7601e73fe5a)] 
18. [arxiv 2504] VLM-R1: A stable and generalizable R1-style Large Vision-Language Model [[Code](https://github.com/om-ai-lab/VLM-R1)] [[Dataset](https://huggingface.co/datasets/omlab/VLM-R1)]
19. [arxiv 2504] VLAA-Thinking: SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models [[Code](https://github.com/UCSC-VLAA/VLAA-Thinking)] [[Dataset](https://huggingface.co/datasets/UCSC-VLAA/VLAA-Thinking)]  
20. [arxiv 2504] MAYE: Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme [[Code](https://github.com/GAIR-NLP/MAYE)] [[Dataset](https://huggingface.co/datasets/ManTle/MAYE)]  
21. [arxiv 2504] R1-SGG: Compile Scene Graphs with Reinforcement Learning [[Code](https://github.com/gpt4vision/R1-SGG)]
22. [arxiv 2504] NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation [[Code](https://github.com/John-AI-Lab/NoisyRollout)] [[Datasets](https://huggingface.co/collections/xyliu6/noisyrollout-67ff992d1cf251087fe021a2)]
23. [arxiv 2504] VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning [[Code](https://github.com/TIGER-AI-Lab/VL-Rethinker)] [[Dataset](https://huggingface.co/datasets/TIGER-Lab/ViRL39K)] 
24. [arxiv 2503] Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning  [[Code](https://github.com/tanhuajie/Reason-RFT)] [[Dataset](https://huggingface.co/datasets/tanhuajie2001/Reason-RFT-CoT-Dataset)]
25. [arxiv 2503] MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning [[Code](https://github.com/ModalMinds/MM-EUREKA)] [[Dataset](https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset)]
26. [arxiv 2503] Visual-RFT: Visual Reinforcement Fine-Tuning [[Code](https://github.com/Liuziyu77/Visual-RFT)] [[Datasets](https://huggingface.co/collections/laolao77/virft-datasets-67bc271b6f2833eccc0651df)] 
27. [arxiv 2503] OpenVLThinker: An Early Exploration to Vision-Language Reasoning via Iterative Self-Improvement [[Code](https://github.com/yihedeng9/OpenVLThinker)]
28. [arxiv 2503] Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning [[Code](https://github.com/minglllli/CLS-RL)] [[Datasets](https://huggingface.co/afdsafas)] 
29. [arxiv 2503] R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization [[Code](https://github.com/jingyi0000/R1-VL)]
30. [arxiv 2503] Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought [[Code](https://github.com/SkyworkAI/Skywork-R1V)]
31. [arxiv 2503] R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization [[Code](https://github.com/Fancy-MLLM/R1-Onevision)] [[Dataset](https://huggingface.co/datasets/Fancy-MLLM/R1-Onevision)] 
32. [arxiv 2503]VisualPRM: An Effective Process Reward Model for Multimodal Reasoning [[Code](https://huggingface.co/OpenGVLab/VisualPRM-8B)]  [[Dataset](https://huggingface.co/datasets/OpenGVLab/VisualPRM400K)]
33. [arxiv 2503] LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL [[Code](https://github.com/TideDra/lmm-r1)]
34. [arxiv 2503] Curr-ReFT: Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning [[Code](https://github.com/ding523/Curr_REFT)] [[Dataset](https://huggingface.co/datasets/ZTE-AIM/Curr-ReFT-data)] 
35. [arxiv 2503] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models [[Code](https://github.com/Osilly/Vision-R1)]
36. [arxiv 2501] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[Code](https://github.com/HJYao00/Mulberry)]
37. [arxiv 2501] Virgo: A Preliminary Exploration on Reproducing o1-like MLLM [[Code](https://github.com/RUCAIBox/Virgo)]
38. [arxiv 2411] Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models [[Code](https://github.com/dongyh20/Insight-V)]
39. [arxiv 2406, ICRA'25] SpatialBot: Precise Spatial Understanding with Vision Language Models  [[Code](https://github.com/BAAI-DCAI/SpatialBot)] [[Dataset](https://huggingface.co/datasets/RussRobin/SpatialQA)]
40. [arxiv 2405, NAACL'25 main] VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models [[Code](https://github.com/RupertLuo/VoCoT)] [[Dataset](https://huggingface.co/datasets/luoruipu1/VoCoT)]



## Video

1. [arxiv 2505] Time-R1: Towards Comprehensive Temporal Reasoning in LLMs  [[Code](https://github.com/ulab-uiuc/Time-R1)] [[Dataset](https://huggingface.co/datasets/ulab-ai/Time-Bench)]\
2. [arxiv 2505] SpaceR: Reinforcing MLLMs in Video Spatial Reasoning [[Code]( https://github.com/OuyangKun10/SpaceR)] [[Dataset](https://huggingface.co/datasets/RUBBISHLIKE/SpaceR-151k)]
3. [arxiv 2504] TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning [[Code](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1)]
4. [arxiv 2504] VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning [[Code](https://github.com/OpenGVLab/VideoChat-R1)]
5. [arxiv 2504] Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning [[Code](https://github.com/OuyangKun10/Spatial-R1)]
6. [arxiv 2504] R1-Zero-VSI: Improved Visual-Spatial Reasoning via R1-Zero-Like Training [[Code](https://github.com/zhijie-group/R1-Zero-VSI)]
7. [arxiv 2503] SEED-Bench-R1: Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1 [[Code](https://github.com/TencentARC/SEED-Bench-R1)] [[Dataset](https://huggingface.co/datasets/TencentARC/SEED-Bench-R1)]
8. [arxiv 2503] Video-R1: Reinforcing Video Reasoning in MLLMs [[Code](https://github.com/tulerfeng/Video-R1)] [[Dataset](https://huggingface.co/datasets/Video-R1/Video-R1-data)] 
9. [arxiv 2503] TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM [[Code](https://github.com/www-Ye/TimeZero)]

   

## Embodied

1. [arxiv 2505] AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving
2. [arxiv 2504, CVPR'25] InteractVLM: 3D Interaction Reasoning from 2D Foundational Models [[Code](https://github.com/kxfan2002/SophiaVL-R1)] [[Dataset](https://huggingface.co/datasets/bunny127/SophiaVL-R1-Thinking-156k)]
3. [arxiv 2504] Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning  [[Code](https://github.com/EmbodiedCity/Embodied-R.code)] 
4. [arxiv 2503] Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks [[Code](https://github.com/zwq2018/embodied_reasoner)] [[Dataset](https://huggingface.co/datasets/zwq2018/embodied_reasoner)] 
5. [arxiv 2503] LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning? [[Code](https://github.com/Tangkexian/LEGO-Puzzles)] [[Dataset](https://huggingface.co/datasets/KexianTang/LEGO-Puzzles)]
6. [arxiv 2503] MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse [[Code](https://github.com/PzySeere/MetaSpatial)] [[Dataset](https://huggingface.co/datasets/zhenyupan/3d_layout_reasoning)] 
7. [arxiv 2502] SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation [[Code](https://github.com/qizekun/SoFar)] 
8. [arxiv 2502] Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos
9. [CVPR'24] ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation [[Code](https://github.com/clorislili/ManipLLM)] 
10. [arxiv 2412] Enhancing Multi-Robot Semantic Navigation Through Multimodal Chain-of-Thought Score Collaboration [[Code](https://github.com/FrankZxShen/MCoCoNav)] 
11. [CVPR'25 Highlight] OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints [[Code will be released soon](https://github.com/pmj110119/OmniManip)]
12. [arxiv 2407, CoRL'24] Robotic Control via Embodied Chain-of-Thought Reasoning [[Code](https://github.com/MichalZawalski/embodied-CoT/)]
13. [arxiv 2305, NIPS'23 Spotlight] EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought [[Code](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch/)]
14. [CoRL'24] ThinkGrasp: A Vision-Language System for Strategic Part Grasping in Clutter [[Code](https://github.com/H-Freax/ThinkGrasp)]
15. SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning
16. Memory-Driven Multimodal Chain of Thought for Embodied Long-Horizon Task Planning



## Math

1. [arxiv 2505] URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics [[Code](https://github.com/URSA-MATH)] [[Datasets](https://huggingface.co/URSA-MATH)] 



## Web/GUI

1. [arxiv 2505] Web-Shepherd: Advancing PRMs for Reinforcing Web Agents  [[Code](https://github.com/kyle8581/Web-Shepherd)] [[Datasets](https://huggingface.co/collections/LangAGI-Lab/web-shepherd-advancing-prms-for-reinforcing-web-agents-682b4f4ad607fc27c4dc49e8)]
2. [arxiv 2505] ARPO: End-to-End Policy Optimization for GUI Agents with Experience Replay [[Code](https://github.com/dvlab-research/ARPO)]
3. [arxiv 2505] GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents [[Code](https://github.com/Yuqi-Zhou/GUI-G1)]
4. [arxiv 2504] WebThinker: Empowering Large Reasoning Models with Deep Research Capability   [[Code](https://github.com/RUC-NLPIR/WebThinker)] [[Dataset](https://huggingface.co/collections/Kangheng/perception-r1-67f6b14f89d307a0ece985af)]
5. [arxiv 2504] InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners [[Code](https://github.com/Reallm-Labs/InfiGUI-R1)]
6. [arxiv 2504] GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents [[Dataset](https://huggingface.co/datasets/ritzzai/GUI-R1)]  [[Code](https://github.com/ritzz-ai/GUI-R1)]



## Medical

1. [arxiv 2503] Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models [[Code](https://github.com/Yuxiang-Lai117/Med-R1)]
2. [arxiv 2502] MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning [[Code](https://huggingface.co/JZPeterPan/MedVLM-R1)]
3. [EMNLP'24 main] MedCoT: Medical Chain of Thought via Hierarchical Expert  [[Code](https://github.com/JXLiu-AI/MedCoT)]



## Grounding

1. [arxiv 2505] UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning [[Code](https://github.com/AMAP-ML/UniVG-R1)] [[Dataset](https://huggingface.co/datasets/GD-ML/UniVG-R1-data)] 
2. [arxiv 2504] CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward [[Code](https://github.com/yeyimilk/CrowdVLM-R1)] [[Dataset](https://huggingface.co/datasets/yeyimilk/CrowdVLM-R1-data)] 
3. [arxiv 2503] Integrating Chain-of-Thought for Multimodal Alignment: A Study on 3D Vision-Language Learning [[Dataset](https://huggingface.co/datasets/Battam/3D-CoT)] 
4. [arxiv 2503] Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement [[Code](https://github.com/dvlab-research/Seg-Zero)]



### Multimodal Reward Model 

1. [arxiv 2505] Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning [[Code](https://github.com/SkyworkAI/Skywork-R1V)]
2. [arxiv 2505] UnifiedReward-Think: Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning [[Code](https://github.com/CodeGoat24/UnifiedReward)] [[Datasets](https://huggingface.co/collections/CodeGoat24/unifiedreward-training-data-67c300d4fd5eff00fa7f1ede)] 
3. [arxiv 2505] R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning [[Code](https://github.com/yfzhang114/r1_reward)] [[Dataset](https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL)]



## Evaluation for reasoning ability

1. [arxiv 2505] SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding  [[Code](https://github.com/haoningwu3639/SpatialScore)]
2. [arxiv 2505] PhyX: Does Your Model Have the "Wits" for Physical Reasoning? [[Code](https://github.com/NastyMarcus/PhyX)] [[Dataset](https://huggingface.co/datasets/Cloudriver/PhyX)]
3. [arxiv 2505] ReasonMap: Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps [[Code](https://github.com/fscdc/ReasonMap)]  [[Dataset](https://huggingface.co/datasets/FSCCS/ReasonMap)] 
4. [arxiv 2504] VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models [[Dataset](https://huggingface.co/datasets/VisuLogic/VisuLogic)] [[Code](https://github.com/VisuLogic-Benchmark)] 
5. [arxiv 2504] Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark [[Dataset](https://huggingface.co/datasets/Enxin/Video-MMLU)] [[Code](https://github.com/Espere-1119-Song/Video-MMLU)] 
6. [arxiv 2504] VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning [[Dataset](https://huggingface.co/datasets/VLM-Reasoning/VCR-Bench)] [[Code](https://github.com/zhishuifeiqian/VCR-Bench)]
7. [arxiv 2504] MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models [[Code](https://github.com/LanceZPF/MDK12)]
8. [arxiv 2503] V1-33K: Toward Multimodal Reasoning by Designing Auxiliary Tasks [[Dataset](https://huggingface.co/datasets/haonan3/V1-33K)] [[Code](https://github.com/haonan3/V1)]
9. [arxiv 2502] MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models [[Dataset](https://huggingface.co/datasets/huanqia/MM-IQ)] [[Code](https://github.com/AceCHQ/MMIQ)] 
10. [arxiv 2502] MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency [[Dataset](https://huggingface.co/datasets/CaraJ/MME-CoT)] [[Code](https://github.com/CaraJ7/MME-CoT)]
11. [arxiv 2502] ZeroBench: An Impossible* Visual Benchmark for Contemporary Large Multimodal Models [[Dataset](https://huggingface.co/datasets/jonathan-roberts1/zerobench)] [[Code](https://github.com/jonathan-roberts1/zerobench/)]
12. [arxiv 2502] HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks [[Dataset](https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark)] [[Code](https://github.com/HumanEval-V/HumanEval-V-Benchmark)]



## Foundation model for reasoning

1. Qwen2.5-VL  [[Code](https://github.com/QwenLM/Qwen2.5-VL)]
2. Bagel  [[Code](https://github.com/bytedance-seed/BAGEL)]
3. BLIP3-o  [[Code](https://github.com/JiuhaiChen/BLIP3o)]
4. Janus-Series: Unified Multimodal Understanding and Generation Models [[Code](https://github.com/deepseek-ai/Janus)]
5. Kimi-VL  [[Code](https://github.com/MoonshotAI/Kimi-VL)]
6. InternVL2-MPO [[Code](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo)]
7. Bytedance ReFT
8. Kimi-K1.5
9. DeepSeek-R1

   

## Code will be released soon

1. Improved Visual-Spatial Reasoning via R1-Zero-Like Training [[Code will be released soon](https://github.com/zhijie-group/R1-Zero-VSI)]
2. WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning [[Code will be released soon](https://github.com/weizhepei/WebAgent-R1)]
3. InteractVLM: 3D Interaction Reasoning from 2D Foundational Models [[Code will be released soon](https://github.com/saidwivedi/InteractVLM)]
4. VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank  [[Code will be released soon](https://github.com/TianheWu/VisualQuality-R1)]
5. Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models  [[Code will be released soon](https://visual-program-distillation.github.io/)]
6. [arxiv 2505] Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner [[Code will be released soon](https://github.com/Wenchuan-Zhang/Patho-R1)]
7. [arxiv 2505] STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs   [[Code will be released soon](https://github.com/zongzhao23/star-r1)]
8. [arxiv 2505] Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning   [[Code will be released soon](https://github.com/maifoundations/Visionary-R1)]
9. [arxiv 2505] R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO [[Code will be released soon](https://github.com/HJYao00/R1-ShareVL)]
10. [arxiv 2505] Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL [[Code will be released soon](https://cof-reasoning.github.io/)]
11. [arxiv 2505] Visual Planning: Let‚Äôs Think Only with Images [[Code will be released soon](https://github.com/yix8/VisualPlanning)]
12. [arxiv 2505] X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains [[Code will be released soon](https://github.com/microsoft/x-reasoner)]
13. [arxiv 2504] Fast-Slow Thinking for Large Vision-Language Model Reasoning [[Code will be released soon](https://github.com/Mr-Loevan/FAST)]
14. [arxiv 2504] Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension [[Code will be released soon](https://github.com/HKUST-LongGroup/Relation-R1)]
15. [arxiv 2503] Q-Insight: Understanding Image Quality via Visual Reinforcement Learning [[Code will be released soon](https://github.com/lwq20020127/Q-Insight)]
16. [arxiv 2503] VisualThinker-R1-Zero: R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model [[Code will be released soon](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)]



## Open-Source Projects (Repos without Paper)

### Training Framework
* [EasyR1 üíª](https://github.com/hiyouga/EasyR1)  ![EasyR1](https://img.shields.io/github/stars/hiyouga/EasyR1) 

### Image
* [R1-V üíª](https://github.com/Deep-Agent/R1-V)  ![R1-V](https://img.shields.io/github/stars/Deep-Agent/R1-V) 
* [Multimodal Open R1 üíª](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)  ![Multimodal Open R1](https://img.shields.io/github/stars/EvolvingLMMs-Lab/open-r1-multimodal)
* [MMR1 üíª](https://github.com/LengSicong/MMR1) ![LengSicong/MMR1](https://img.shields.io/github/stars/LengSicong/MMR1) 
* [R1-Multimodal-Journey üíª](https://github.com/FanqingM/R1-Multimodal-Journey) ![R1-Multimodal-Journey](https://img.shields.io/github/stars/FanqingM/R1-Multimodal-Journey) 
* [R1-Vision üíª](https://github.com/yuyq96/R1-Vision) ![R1-Vision](https://img.shields.io/github/stars/yuyq96/R1-Vision) 
* [Ocean-R1 üíª](https://github.com/VLM-RL/Ocean-R1)  ![Ocean-R1](https://img.shields.io/github/stars/VLM-RL/Ocean-R1) 
* [R1V-Free üíª](https://github.com/Exgc/R1V-Free)  ![Exgc/R1V-Free](https://img.shields.io/github/stars/Exgc/R1V-Free)
* [SeekWorld üíª](https://github.com/TheEighthDay/SeekWorld)  ![TheEighthDay/SeekWorld](https://img.shields.io/github/stars/TheEighthDay/SeekWorld) 
* [R1-Track üíª](https://github.com/Wangbiao2/R1-Track)  ![Wangbiao2/R1-Track](https://img.shields.io/github/stars/Wangbiao2/R1-Track)

### Video

* [Open R1 Video üíª](https://github.com/Wang-Xiaodong1899/Open-R1-Video) ![Open R1 Video](https://img.shields.io/github/stars/Wang-Xiaodong1899/Open-R1-Video) 
* [Temporal-R1 üíª](https://github.com/appletea233/Temporal-R1)  ![Temporal-R1](https://img.shields.io/github/stars/appletea233/Temporal-R1) 
* [Open-LLaVA-Video-R1 üíª](https://github.com/Hui-design/Open-LLaVA-Video-R1) ![Open-LLaVA-Video-R1](https://img.shields.io/github/stars/Hui-design/Open-LLaVA-Video-R1) 

### Agent 

* [VAGEN üíª](https://github.com/RAGEN-AI/VAGEN) ![VAGEN](https://img.shields.io/github/stars/RAGEN-AI/VAGEN)



## Acknowledgement

1. https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs
2. https://github.com/yaotingwangofficial/Awesome-MCoT
3. https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln
4. https://modelscope.cn/studios/AI-ModelScope/awesome-reasoning



## Citation

If you find this repository useful for your research and applications, please star us and consider citing:

```
@misc{Tang2025Awesome-spatial-visual-reasoning-MLLMs,
  title={Awesome-spatial-visual-reasoning-MLLMs},
  author={Jing Tang},
  year={2025},
  howpublished={\url{https://github.com/vaew/Awesome-spatial-visual-reasoning-MLLMs}},
  note={Github Repository},
}
```
