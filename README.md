# Awesome-spatial-visual-reasoning-MLLMs

Repository for awesome spatial/visual reasoning MLLMs.  (focus more on embodied applications)

![Research Topics in Spatial-Visual Reasoning](wordcloud.png)

## Image

1. [arxiv 2505] Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning [[Code](https://github.com/dongguanting/Tool-Star)] 
2. [arxiv 2505] Active-o3 : Empowering Multimodal Large Language Models with Active Perception via GRPO [[Code](https://github.com/aim-uofa/Active-o3)] 
3. [arxiv 2505] One RL to See Them All: Visual Triple Unified Reinforcement Learning [[Code](https://github.com/MiniMax-AI/One-RL-to-See-Them-All)] [[Dataset](https://huggingface.co/datasets/One-RL-to-See-Them-All/Orsta-Data-47k)] 
4. [arxiv 2505] DeepEyes: Incentivizing ‚ÄúThinking with Images‚Äù via Reinforcement Learning [[Code](https://github.com/Visual-Agent/DeepEyes)] [[Dataset](https://huggingface.co/datasets/ChenShawn/DeepEyes-Datasets-47k)]
5. [arxiv 2505] R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO [[Code](https://github.com/HJYao00/R1-ShareVL)]
6. [arxiv 2505]  Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models [[Code](https://github.com/kokolerk/TON)] [[Datasets](https://huggingface.co/collections/kolerk/ton-682ad9038395c21e228a645b)]  
7. [arxiv2505] Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning [[Datasets](https://huggingface.co/collections/TIGER-Lab/pixel-reasoner-682fe96ea946d10dda60d24e)] [[Code](https://github.com/TIGER-AI-Lab/Pixel-Reasoner)]
8. [arxiv 2505] VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction [[Code](https://github.com/VITA-Group/VLM-3R)]
9. [arxiv 2025] G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning [[Code](https://github.com/chenllliang/G1)]
10. [arxiv 2505] SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward  [[Code](https://github.com/kxfan2002/SophiaVL-R1)] [[Dataset](https://huggingface.co/datasets/bunny127/SophiaVL-R1-Thinking-156k)]
11. [arxiv 2505] VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning [[Code](https://github.com/dvlab-research/VisionReasoner)] [[Dataset](https://huggingface.co/datasets/Ricky06662/VisionReasoner_multi_object_7k_840)]
12. [arxiv 2505] GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning [[Code](https://github.com/yueliu1999/GuardReasoner-VL)]
13. [arxiv 2505] Multi-SpatialMLLM Multi-Frame Spatial Understanding with Multi-Modal Large Language Models  [[Code](https://github.com/facebookresearch/Multi-SpatialMLLM)] 
14. [arxiv 2505] GRIT: Teaching MLLMs to Think with Images [[Code](https://github.com/eric-ai-lab/GRIT)] 
15. [arxiv 2505] Visual Agentic Reinforcement Fine-Tuning [[Code](https://github.com/Liuziyu77/Visual-RFT)] [[Dataset](https://huggingface.co/collections/laolao77/visual-arft-682c601d0e35ac6470adfe9f)]
16. [arxiv 2505] OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning  [[Code](https://github.com/zhaochen0110/OpenThinkIMG)] [[Dataset](https://huggingface.co/collections/Warrieryes/openthinkimg-68244a63e97a24d9b7ffcde9)]
17. [arxiv 2504] Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning [[Code](https://github.com/SkyworkAI/Skywork-R1V)]
18. [arxiv 2504] SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM [[HuggingFace](https://huggingface.co/Kwaipilot/SRPO-Qwen-32B)]
19. [arxiv 2504] Perception-R1: Pioneering Perception Policy with Reinforcement Learning  [[Code](https://github.com/linkangheng/PR1)] [[Dataset](https://huggingface.co/collections/Kangheng/perception-r1-67f6b14f89d307a0ece985af)]
20. [arxiv 2504] SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement  [[Code](https://github.com/si0wang/ThinkLite-VL)] [[Datasets](https://huggingface.co/collections/russwang/thinklite-vl-67f88c6493f8a7601e73fe5a)] 
21. [arxiv 2504] VLM-R1: A stable and generalizable R1-style Large Vision-Language Model [[Code](https://github.com/om-ai-lab/VLM-R1)] [[Dataset](https://huggingface.co/datasets/omlab/VLM-R1)]
22. [arxiv 2504] VLAA-Thinking: SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models [[Code](https://github.com/UCSC-VLAA/VLAA-Thinking)] [[Dataset](https://huggingface.co/datasets/UCSC-VLAA/VLAA-Thinking)]  
23. [arxiv 2504] MAYE: Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme [[Code](https://github.com/GAIR-NLP/MAYE)] [[Dataset](https://huggingface.co/datasets/ManTle/MAYE)]  
24. [arxiv 2504] R1-SGG: Compile Scene Graphs with Reinforcement Learning [[Code](https://github.com/gpt4vision/R1-SGG)]
25. [arxiv 2504] NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation [[Code](https://github.com/John-AI-Lab/NoisyRollout)] [[Datasets](https://huggingface.co/collections/xyliu6/noisyrollout-67ff992d1cf251087fe021a2)]
26. [arxiv 2504] VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning [[Code](https://github.com/TIGER-AI-Lab/VL-Rethinker)] [[Dataset](https://huggingface.co/datasets/TIGER-Lab/ViRL39K)] 
27. [arxiv 2503] Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning  [[Code](https://github.com/tanhuajie/Reason-RFT)] [[Dataset](https://huggingface.co/datasets/tanhuajie2001/Reason-RFT-CoT-Dataset)]
28. [arxiv 2503] MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning [[Code](https://github.com/ModalMinds/MM-EUREKA)] [[Dataset](https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset)]
29. [arxiv 2503] Visual-RFT: Visual Reinforcement Fine-Tuning [[Code](https://github.com/Liuziyu77/Visual-RFT)] [[Datasets](https://huggingface.co/collections/laolao77/virft-datasets-67bc271b6f2833eccc0651df)] 
30. [arxiv 2503] OpenVLThinker: An Early Exploration to Vision-Language Reasoning via Iterative Self-Improvement [[Code](https://github.com/yihedeng9/OpenVLThinker)]
31. [arxiv 2503] Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning [[Code](https://github.com/minglllli/CLS-RL)] [[Datasets](https://huggingface.co/afdsafas)] 
32. [arxiv 2503] R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization [[Code](https://github.com/jingyi0000/R1-VL)]
33. [arxiv 2503] Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought [[Code](https://github.com/SkyworkAI/Skywork-R1V)]
34. [arxiv 2503] R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization [[Code](https://github.com/Fancy-MLLM/R1-Onevision)] [[Dataset](https://huggingface.co/datasets/Fancy-MLLM/R1-Onevision)] 
35. [arxiv 2503, CVPR'25] CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models [[Code](https://github.com/THUNLP-MT/CoSpace)]
36. [arxiv 2503]VisualPRM: An Effective Process Reward Model for Multimodal Reasoning [[Code](https://huggingface.co/OpenGVLab/VisualPRM-8B)]  [[Dataset](https://huggingface.co/datasets/OpenGVLab/VisualPRM400K)]
37. [arxiv 2503] LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL [[Code](https://github.com/TideDra/lmm-r1)]
38. [arxiv 2503] Curr-ReFT: Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning [[Code](https://github.com/ding523/Curr_REFT)] [[Dataset](https://huggingface.co/datasets/ZTE-AIM/Curr-ReFT-data)] 
39. [arxiv 2503] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models [[Code](https://github.com/Osilly/Vision-R1)]
40. [arxiv 2502, NAACL'25 findings] Mitigating Hallucinations in Multimodal Spatial Relations through
    Constraint-Aware Prompting [[Code](https://github.com/jwu114/CAP)]
41. [arxiv 2502] VADAR: Visual Agentic AI for Spatial Reasoning with a Dynamic API [[Code](https://github.com/damianomarsili/VADAR)] [[Dataset](https://huggingface.co/datasets/dmarsili/Omni3D-Bench)] 
42. [arxiv 2502] Introducing Visual Perception Token into Multimodal Large Language Model [[Code](https://github.com/yu-rp/VisualPerceptionToken)]
43. [arxiv 2501] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[Code](https://github.com/HJYao00/Mulberry)]
44. [arxiv 2501] Virgo: A Preliminary Exploration on Reproducing o1-like MLLM [[Code](https://github.com/RUCAIBox/Virgo)]
45. [arxiv 2412, CVPR'25] Perception Tokens Enhance Visual Reasoning in Multimodal Language Models [[Code](https://github.com/mahtabbigverdi/Aurora-perception)]
46. [arxiv 2411] Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models [[Code](https://github.com/dongyh20/Insight-V)]
47. [arxiv 2410] Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions [[Code](https://github.com/euclid-multimodal/Euclid)] [[Dataset](https://huggingface.co/euclid-multimodal)] 
48. [arxiv 2410, ICLR'25 Oral] Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities [[Code](https://github.com/sled-group/COMFORT)] [[Dataset](https://huggingface.co/datasets/sled-umich/COMFORT)]
49. [arxiv 2406, ICRA'25] SpatialBot: Precise Spatial Understanding with Vision Language Models  [[Code](https://github.com/BAAI-DCAI/SpatialBot)] [[Dataset](https://huggingface.co/datasets/RussRobin/SpatialQA)]
50. [arxiv 2406, NIPS'24] SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models [[Code](https://github.com/AnjieCheng/SpatialRGPT)] [[Dataset](https://huggingface.co/datasets/a8cheng/OpenSpatialDataset)]
51. [NIPS'24] Right this way: Can VLMs Guide Us to See More to Answer Questions? [[Code](https://github.com/LeoLee7/Directional_guidance)] [[Dataset](https://huggingface.co/datasets/LeoLee7/Directional_Guidance)]
52. [arxiv 2405, NAACL'25 main] VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models [[Code](https://github.com/RupertLuo/VoCoT)] [[Dataset](https://huggingface.co/datasets/luoruipu1/VoCoT)]
53. [arxiv 2403] Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models [[Code](https://github.com/dongyh20/Chain-of-Spot)]
54. [arxiv 2401,CVPR'24 main] SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities [[Code](https://github.com/remyxai/VQASynth)]
55. [ACM MM'24] LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial Description [[Code](https://github.com/LLLogen/VSDcode)] [[Dataset](https://huggingface.co/datasets/swordli/LLaVA-VSD-120K)]



## Video

1. [arxiv 2505] Time-R1: Towards Comprehensive Temporal Reasoning in LLMs  [[Code](https://github.com/ulab-uiuc/Time-R1)] [[Dataset](https://huggingface.co/datasets/ulab-ai/Time-Bench)]

2. [arxiv 2505] SpaceR: Reinforcing MLLMs in Video Spatial Reasoning [[Code]( https://github.com/OuyangKun10/SpaceR)] [[Dataset](https://huggingface.co/datasets/RUBBISHLIKE/SpaceR-151k)]

3. [arxiv 2504] TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning [[Code](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1)]

4. [arxiv 2504] VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning [[Code](https://github.com/OpenGVLab/VideoChat-R1)]

5. [arxiv 2504] Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning [[Code](https://github.com/OuyangKun10/Spatial-R1)]

6. [arxiv 2504] R1-Zero-VSI: Improved Visual-Spatial Reasoning via R1-Zero-Like Training [[Code](https://github.com/zhijie-group/R1-Zero-VSI)]

7. [arxiv 2503] SEED-Bench-R1: Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1 [[Code](https://github.com/TencentARC/SEED-Bench-R1)] [[Dataset](https://huggingface.co/datasets/TencentARC/SEED-Bench-R1)]

8. [arxiv 2503] Video-R1: Reinforcing Video Reasoning in MLLMs [[Code](https://github.com/tulerfeng/Video-R1)] [[Dataset](https://huggingface.co/datasets/Video-R1/Video-R1-data)] 

9. [arxiv 2503] TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM [[Code](https://github.com/www-Ye/TimeZero)]

10. [arxiv 2412, CVPR'25] Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene
    Understanding [[Code](https://github.com/LaVi-Lab/Video-3D-LLM)] [[Dataset](https://huggingface.co/datasets/zd11024/Video-3D-LLM_data)] 

    

## Embodied

1. [arxiv 2506] Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning [[Code](https://github.com/CHEN-H01/Fast-in-Slow)]
2. [arxiv 2506] Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces  [[Code will be released soon](https://github.com/OpenGVLab/VeBrain)]
3. [arxiv 2505] Learning 3D Persistent Embodied World Models
4. [arxiv 2505] ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge [[Code will be released soon](https://chatvla-2.github.io/)]
5. [arxiv 2505] Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents [[Code will be released soon](https://agentic-robot.github.io/)]
6. [arxiv 2505] InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning [[Code](https://github.com/Koorye/Inspire)] 
7. [arxiv 2505] From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems [[Code will be released soon](https://github.com/xiuchao/InstructionGrounding)]
8. [arxiv 2505] OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning [[Code](https://github.com/Fanqi-Lin/OneTwoVLA)] [[Dataset](https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset)]
9. [arxiv 2505] VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning [[Code](https://github.com/GuanxingLu/vlarl)] 
10. [arxiv 2505] AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving
11. [arxiv 2505] SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning [[Code](https://github.com/yliu-cs/SSR)] [[Dataset](https://huggingface.co/collections/yliu-cs/ssr-682d44496b64e4edd94092bb)]
12. [arxiv 2504, CVPR'25] InteractVLM: 3D Interaction Reasoning from 2D Foundational Models [[Code](https://github.com/kxfan2002/SophiaVL-R1)] [[Dataset](https://huggingface.co/datasets/bunny127/SophiaVL-R1-Thinking-156k)]
13. [arxiv 2504] Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning  [[Code](https://github.com/EmbodiedCity/Embodied-R.code)] 
14. [arxiv 2504, CVPR'25] RoboGround: Robotic Manipulation with Grounded Vision-Language Priors   [[Code](https://github.com/ZzZZCHS/RoboGround)] [[Dataset](https://huggingface.co/datasets/ZzZZCHS/RoboGround_Data/tree/main)]
15. [arxiv 2504] SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning [[Code](https://spatial-reasoner.github.io/)] 
16. [arxiv 2504] RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics
17. [arxiv 2503] PhysVLM: Enabling Visual Language Models to Understand Robotic Physical
    Reachability [[Code](https://github.com/unira-zwj/PhysVLM)] [[Dataset](https://huggingface.co/JettZhou/PhysVLM-Qwen2.5-3B)]
18. [arxiv 2503, CVPR'25] CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models [[Code will be releasd soon](https://cot-vla.github.io/)] 
19. [arxiv 2503] Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks [[Code](https://github.com/zwq2018/embodied_reasoner)] [[Dataset](https://huggingface.co/datasets/zwq2018/embodied_reasoner)] 
20. [arxiv 2503] LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning? [[Code](https://github.com/Tangkexian/LEGO-Puzzles)] [[Dataset](https://huggingface.co/datasets/KexianTang/LEGO-Puzzles)]
21. [arxiv 2503] MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse [[Code](https://github.com/PzySeere/MetaSpatial)] [[Dataset](https://huggingface.co/datasets/zhenyupan/3d_layout_reasoning)] 
22. [arxiv 2503] Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning [[Code](https://github.com/nvidia-cosmos/cosmos-reason1)] [[Dataset](https://huggingface.co/collections/nvidia/cosmos-reason1-67c9e926206426008f1da1b7)] 
23. [arxiv 2502, CVPR'25] RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete [[Code](https://github.com/FlagOpen/RoboBrain/)]
24. [arxiv 2502, ICRA'25] A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards [[Code](https://github.com/shivanshpatel35/IKER)]
25. [ICRA'25] UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation  [[Code](https://gpt-affordance.github.io/)]
26. [arxiv 2502] ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy [[Code](https://github.com/cccedric/conrft)] 
27. [arxiv 2502] SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation [[Code](https://github.com/qizekun/SoFar)] 
28. [arxiv 2502] Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos
29. [arxiv 2502, ICLR'25] Predicate Hierarchies Improve Few-Shot State Classification [[Code](https://github.com/emilyzjin/phier)] 
30. [arxiv 2501, RSS'25] SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Models [[Code](https://github.com/SpatialVLA/SpatialVLA)] 
31. [ICLR'25] Vision Language Models are In-Context Value Learners [[Code will be released soon](https://generative-value-learning.github.io/)] 
32. [CVPR'24] ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation [[Code](https://github.com/clorislili/ManipLLM)] 
33. [arxiv 2412] RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World [[Code](https://github.com/WayneMao/RoboMatrix)] [[Dataset](https://huggingface.co/datasets/WayneMao/RoboMatrix)]
34. [arxiv 2412, RSS'25] NaVILA: Legged Robot Vision-Language-Action Model for Navigation [[Code](https://github.com/yang-zj1026/legged-loco)] 
35. [arxiv 2412] Enhancing Multi-Robot Semantic Navigation Through Multimodal Chain-of-Thought Score Collaboration [[Code](https://github.com/FrankZxShen/MCoCoNav)] 
36. [arxiv 2412] Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection [[Code will be released soon](https://zhoues.github.io/Code-as-Monitor/)]
37. [arxiv 2412, CVPR'25] 3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning [[Code](https://github.com/UMass-Embodied-AGI/3D-Mem)]
38. [CoRL'24 LEAP workshop] SkillWrapper: Autonomously Learning Interpretable Skill Abstractions with Foundation Models [[Code](https://github.com/YzyLmc/skill_wrapper)]
39. [arxiv 2411, CVPR'25 Oral] RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics [[Code](https://github.com/NVlabs/RoboSpatial)] [[Dataset](https://huggingface.co/datasets/chanhee-luke/RoboSpatial-Home)]
40. [arxiv 2410,CoRL'24] Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models [[Code](https://github.com/intuitive-robots/NILS)] [[Dataset](https://huggingface.co/collections/holgerson/nils-datasets-671b4cc18750e04558d7f98f)]
41. [arxiv 2410, ICLR'25] AHA: A Vision-Language-Model for Detecting and Reasoning over Failures in Robotic Manipulation [[Code](https://github.com/NVlabs/AHA)] 
42. [arxiv 2409, RAL'25] MotIF: Motion Instruction Fine-tuning [[Code](https://github.com/Minyoung1005/motif)]
43. [CVPR'25 Highlight] OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints [[Code will be released soon](https://github.com/pmj110119/OmniManip)]
44. [arxiv 2407, CoRL'24] Robotic Control via Embodied Chain-of-Thought Reasoning [[Code](https://github.com/MichalZawalski/embodied-CoT/)]
45. [arxiv 2406, CoRL'24] RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics [[Code](https://github.com/wentaoyuan/RoboPoint)] [[Dataset](https://huggingface.co/datasets/wentao-yuan/robopoint-data)]
46. [arxiv 2406, EMNLP'24 main] TopViewRS: Vision-Language Models as Top-View Spatial Reasoners [[Code](https://github.com/cambridgeltl/topviewrs)] [[Dataset](https://huggingface.co/datasets/chengzu/topviewrs)]
47. [arxiv 2406, ICLR'25] LLaRA: Supercharging Robot Learning Data for Vision-Language Policy [[Code](https://github.com/LostXine/LLaRA)]
48. [arxiv 2405] ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation [[Code](https://github.com/huangwl18/ReKep)]
49. [arxiv 2403, NIPS'24] SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors [[Code](https://github.com/dannymcy/zeroshot_task_hallucination_code)]
50. [arxiv 2402, COLING'24 main] Scaffolding Coordinates to Promote Vision-Language Coordination
    in Large Multi-Modal Models [[Code](https://github.com/THUNLP-MT/Scaffold)]
51. [arxiv 2305, NIPS'23 Spotlight] EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought [[Code](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch/)]
52. [CoRL'24] ThinkGrasp: A Vision-Language System for Strategic Part Grasping in Clutter [[Code](https://github.com/H-Freax/ThinkGrasp)]
53. [RAL'24] GPT-4V(ision) for Robotics: Multimodal Task Planning From Human Demonstration
54. [NIPS'24 Spotlight] ICAL: VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought [[Code](https://github.com/Gabesarch/ICAL)]
55. SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning
56. Memory-Driven Multimodal Chain of Thought for Embodied Long-Horizon Task Planning
57. [arxiv 2306,CoRL'23] REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction  [[Code](https://github.com/real-stanford/reflect)] [[Dataset](https://www.cs.columbia.edu/~liuzeyi/reflect_data/)]



## Audio/Omini

1. [arxiv 2505] Omni-R1 (ZJU): Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration [[Code](https://github.com/aim-uofa/Omni-R1)][]
2. [arxiv 2505] Omni-R1 (MIT): Do You Really Need Audio to Fine-Tune Your Audio LLM?
3. [arxiv 2505] EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning [[Dataset](https://huggingface.co/datasets/harryhsing/OmniInstruct_V1_AVQA_R1)] [[Code](https://github.com/HarryHsing/EchoInk)]
4. [arxiv 2504] SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning
5. [arxiv 2503] R1-AQA: Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering [[Code](https://github.com/xiaomi-research/r1-aqa)]
6. [arxiv 2503] Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models [[Code](https://github.com/xzf-thu/Audio-Reasoner)]
7. [arxiv 2503] R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning  [[Code](https://github.com/HumanMLLM/R1-Omni)]
8. [arxiv 2402, ICML'24] BAT: Learning to Reason about Spatial Sounds with Large Language Models [[Code](https://github.com/zszheng147/Spatial-AST)] [[Datasets](https://huggingface.co/datasets/zhisheng01/SpatialAudio)] 


## Math

1. [arxiv 2505] URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics [[Code](https://github.com/URSA-MATH)] [[Datasets](https://huggingface.co/URSA-MATH)] 
1. [arxiv 2403, COLM'24] How Far Are We from Intelligent Visual Deductive Reasoning? [[Code](https://github.com/apple/ml-rpm-bench)]



## Web/GUI

1. [arxiv 2505] Web-Shepherd: Advancing PRMs for Reinforcing Web Agents  [[Code](https://github.com/kyle8581/Web-Shepherd)] [[Datasets](https://huggingface.co/collections/LangAGI-Lab/web-shepherd-advancing-prms-for-reinforcing-web-agents-682b4f4ad607fc27c4dc49e8)]
2. [arxiv 2505] ARPO: End-to-End Policy Optimization for GUI Agents with Experience Replay [[Code](https://github.com/dvlab-research/ARPO)]
3. [arxiv 2505] GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents [[Code](https://github.com/Yuqi-Zhou/GUI-G1)]
4. [arxiv 2504] WebThinker: Empowering Large Reasoning Models with Deep Research Capability   [[Code](https://github.com/RUC-NLPIR/WebThinker)] [[Dataset](https://huggingface.co/collections/Kangheng/perception-r1-67f6b14f89d307a0ece985af)]
5. [arxiv 2504] InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners [[Code](https://github.com/Reallm-Labs/InfiGUI-R1)]
6. [arxiv 2504] GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents [[Dataset](https://huggingface.co/datasets/ritzzai/GUI-R1)]  [[Code](https://github.com/ritzz-ai/GUI-R1)]



## Medical

1. [arxiv 2503] Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models [[Code](https://github.com/Yuxiang-Lai117/Med-R1)]
2. [arxiv 2502] MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning [[Code](https://huggingface.co/JZPeterPan/MedVLM-R1)]
3. [EMNLP'24 main] MedCoT: Medical Chain of Thought via Hierarchical Expert  [[Code](https://github.com/JXLiu-AI/MedCoT)]



## Grounding

1. [arxiv 2505] UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning [[Code](https://github.com/AMAP-ML/UniVG-R1)] [[Dataset](https://huggingface.co/datasets/GD-ML/UniVG-R1-data)] 
2. [arxiv 2504] CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward [[Code](https://github.com/yeyimilk/CrowdVLM-R1)] [[Dataset](https://huggingface.co/datasets/yeyimilk/CrowdVLM-R1-data)] 
3. [arxiv 2503] Integrating Chain-of-Thought for Multimodal Alignment: A Study on 3D Vision-Language Learning [[Dataset](https://huggingface.co/datasets/Battam/3D-CoT)] 
4. [arxiv 2502, CVPR'25 Highlight] Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models [[Code](https://github.com/honda-research-institute/Anomaly-OneVision)]
5. [arxiv 2503] Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement [[Code](https://github.com/dvlab-research/Seg-Zero)]
6. [arxiv 2403, ECCV'24] DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM [[Code](https://github.com/yixuan730/DetToolChain)]
7. [arxiv 2403, ECCV'24] Agent3D-Zero: An Agent for Zero-shot 3D Understanding



### Multimodal Reward Model 

1. [arxiv 2505] Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning [[Code](https://github.com/SkyworkAI/Skywork-R1V)]
2. [arxiv 2505] UnifiedReward-Think: Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning [[Code](https://github.com/CodeGoat24/UnifiedReward)] [[Datasets](https://huggingface.co/collections/CodeGoat24/unifiedreward-training-data-67c300d4fd5eff00fa7f1ede)] 
3. [arxiv 2505] R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning [[Code](https://github.com/yfzhang114/r1_reward)] [[Dataset](https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL)]



## Evaluation for reasoning ability

1. [arxiv 2505] ManiTaskGen: A Comprehensive Task Generator for Benchmarking and Improving Vision-Language Agents on Embodied Decision-Making
2. [arxiv 2505] Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets [[Dataset](https://huggingface.co/datasets/keplerccc/Robo2VLM-1)]
3. [arxiv 2505] RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction [[Code](https://github.com/MINT-SJTU/RoboFAC)]
4. [arxiv 2505] SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding  [[Code](https://github.com/haoningwu3639/SpatialScore)]
5. [arxiv 2505] EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video [[Dataset](https://ml-site.cdn-apple.com/datasets/egodex/test.zip)]
6. [arxiv 2505] PhyX: Does Your Model Have the "Wits" for Physical Reasoning? [[Code](https://github.com/NastyMarcus/PhyX)] [[Dataset](https://huggingface.co/datasets/Cloudriver/PhyX)]
7. [arxiv 2505] ReasonMap: Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps [[Code](https://github.com/fscdc/ReasonMap)]  [[Dataset](https://huggingface.co/datasets/FSCCS/ReasonMap)] 
8. [arxiv 2504] VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models [[Dataset](https://huggingface.co/datasets/VisuLogic/VisuLogic)] [[Code](https://github.com/VisuLogic-Benchmark)] 
9. [arxiv 2504] Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark [[Dataset](https://huggingface.co/datasets/Enxin/Video-MMLU)] [[Code](https://github.com/Espere-1119-Song/Video-MMLU)] 
10. [arxiv 2504] VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning [[Dataset](https://huggingface.co/datasets/VLM-Reasoning/VCR-Bench)] [[Code](https://github.com/zhishuifeiqian/VCR-Bench)]
11. [arxiv 2504] MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models [[Code](https://github.com/LanceZPF/MDK12)]
12. [arxiv 2504] Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs  [[Dataset](https://huggingface.co/datasets/ch-chenyu/All-Angles-Bench)] [[Code](https://github.com/Chenyu-Wang567/All-Angles-Bench)]
13. [arxiv 2503] Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models [[Code](https://github.com/stogiannidis/srbench)] 
14. [arxiv 2503] STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding? [[Dataset](https://huggingface.co/datasets/MINT-SJTU/STI-Bench)] [[Code](https://github.com/MINT-SJTU/STI-Bench)] 
15. [arxiv 2503] Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space [[Code](https://github.com/WeichenZh/Open3DVQA)]
16. [arxiv 2503] Gemini Robotics: Bringing AI into the Physical World (Embodied Reasoning QA Evaluation Dataset) [[Code](https://github.com/embodiedreasoning/ERQA)]
17. [arxiv 2503] V1-33K: Toward Multimodal Reasoning by Designing Auxiliary Tasks [[Dataset](https://huggingface.co/datasets/haonan3/V1-33K)] [[Code](https://github.com/haonan3/V1)]
18. [arxiv 2502] Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models [[Dataset](https://huggingface.co/datasets/RyanWW/Spatial457)] [[Code](https://github.com/XingruiWang/Spatial457?tab=readme-ov-file)]
19. [arxiv 2502] MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models [[Dataset](https://huggingface.co/datasets/huanqia/MM-IQ)] [[Code](https://github.com/AceCHQ/MMIQ)] 
20. [arxiv 2502] MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency [[Dataset](https://huggingface.co/datasets/CaraJ/MME-CoT)] [[Code](https://github.com/CaraJ7/MME-CoT)]
21. [arxiv 2502] ZeroBench: An Impossible* Visual Benchmark for Contemporary Large Multimodal Models [[Dataset](https://huggingface.co/datasets/jonathan-roberts1/zerobench)] [[Code](https://github.com/jonathan-roberts1/zerobench/)]
22. [arxiv 2502] HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks [[Dataset](https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark)] [[Code](https://github.com/HumanEval-V/HumanEval-V-Benchmark)]
23. [arxiv 2501] EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents  [[Dataset](https://huggingface.co/datasets/EmbodiedEval/EmbodiedEval)] [[Code](https://github.com/thunlp/EmbodiedEval)]
24. [ICLR 2024 & ECCV 2024] The All-Seeing Projects: Towards Panoptic Visual Recognition&Understanding and General Relation Comprehension of the Open World [[Code](https://github.com/OpenGVLab/all-seeing)]
25. [arxiv 2411, EMNLP'24 main] Spatial-MM: An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models [[Code](https://github.com/FatemehShiri/Spatial-MM)]
26. [arxiv 2411] GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks [[Code](https://github.com/The-AI-Alliance/GEO-Bench-VLM)]
27. [arxiv 2412] Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces [[Dataset](https://huggingface.co/datasets/nyu-visionx/VSI-Bench)]
28. [arxiv 2409] FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension [[Code](https://github.com/liujunzhuo/FineCops-Ref)]
29. [arxiv 2306] LIBERO:Benchmarking Knowledge Transfer for Lifelong Robot Learning [[Code](https://github.com/Lifelong-Robot-Learning/LIBERO)]
30. [RAL'22 ] CALVIN: A benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks [[Code](https://github.com/mees/calvin)]
31. [arxiv 2210, ICML'23] VIMA: General Robot Manipulation with Multimodal Prompts [[Dataset](https://huggingface.co/datasets/VIMA/VIMA-Data)] [[Code](https://github.com/vimalabs/VIMA)]



## Embodied Environment

1. Maniskill [[Code](https://github.com/haosulab/ManiSkill)]
2. Issac Lab/Gym/Sim [[Code](https://github.com/isaac-sim/IsaacGymEnvs)]
3. SimplerEnv [[Code](https://github.com/isaac-sim/IsaacGymEnvs)]
4. Roboverse [[code](https://github.com/simpler-env/SimplerEnv)]
5. Bullet Physics SDK [[code](https://github.com/bulletphysics/bullet3)]
6. Genesis [[Code](https://github.com/Genesis-Embodied-AI/Genesis)]
7. RoboTwin [[Code](https://github.com/TianxingChen/RoboTwin)]
8. RLBench [[Code](https://github.com/stepjam/RLBench)]
9. OmniGibson [[Document](https://behavior.stanford.edu/omnigibson/getting_started/installation.html)]
10.  AI2-THOR [[Link](https://ai2thor.allenai.org/)]
11. Legent [[code](https://github.com/thunlp/LEGENT)]
12. Gazebo [[Link](https://gazebosim.org/)]
13. CoppeliaSim (formerly known as V-REP)  [[Link](https://www.coppeliarobotics.com/)]



## Foundation model for reasoning

1. Qwen2.5-VL  [[Code](https://github.com/QwenLM/Qwen2.5-VL)]
2. Bagel  [[Code](https://github.com/bytedance-seed/BAGEL)]
3. BLIP3-o  [[Code](https://github.com/JiuhaiChen/BLIP3o)]
4. Janus-Series: Unified Multimodal Understanding and Generation Models [[Code](https://github.com/deepseek-ai/Janus)]
5. Kimi-VL  [[Code](https://github.com/MoonshotAI/Kimi-VL)]
6. InternVL3 [[code](https://github.com/OpenGVLab/InternVL)]
7. Bytedance ReFT
8. Kimi-K1.5
9. DeepSeek-R1

   

## Code will be released soon

1. [arxiv 2505, CVPR'25 highlight] SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models
2. [ICLR'25] MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents [[Code will be released soon](https://github.com/PKU-RL/MART)] 
3. [ICLR'25] Chain-of-region: Visual Language Models Need Details for Diagram Analysis
4. [arxiv 2505, ICLR'25] Chain-of-Focus Prompting: Leveraging Sequential Visual Cues to Prompt Large Autoregressive Vision Models [[Code will be released soon](https://cof-reasoning.github.io/)] 
5. [arxiv 2505] 3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model [[Code will be released soon](https://3dllm-mem.github.io/)]
6. [arxiv 2505] PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes [[Code will be released soon](https://nianticlabs.github.io/placeit3d/)]
7. [arxiv 2505] WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning [[Code will be released soon](https://github.com/weizhepei/WebAgent-R1)]
8. [arxiv 2505] VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank [[Code will be released soon](https://github.com/TianheWu/VisualQuality-R1)]
9. [arxiv 2505] Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner [[Code will be released soon](https://github.com/Wenchuan-Zhang/Patho-R1)]
10. [arxiv 2505] STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs   [[Code will be released soon](https://github.com/zongzhao23/star-r1)]
11. [arxiv 2505] Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning   [[Code will be released soon](https://github.com/maifoundations/Visionary-R1)]
12. [arxiv 2505] R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO [[Code will be released soon](https://github.com/HJYao00/R1-ShareVL)]
13. [arxiv 2505] Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL [[Code will be released soon](https://cof-reasoning.github.io/)]
14. [arxiv 2505] Visual Planning: Let‚Äôs Think Only with Images [[Code will be released soon](https://github.com/yix8/VisualPlanning)]
15. [arxiv 2505] X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains [[Code will be released soon](https://github.com/microsoft/x-reasoner)]
16. [arxiv 2505] ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation [[Code will be released soon](https://manipbench.github.io/)]
17. [arxiv 2504, ICRA'25] Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models [[Code will be released soon](https://chain-of-modality.github.io/)]
18. [arxiv 2504] InteractVLM: 3D Interaction Reasoning from 2D Foundational Models [[Code will be released soon](https://github.com/saidwivedi/InteractVLM)]
19. [arxiv 2504] Fast-Slow Thinking for Large Vision-Language Model Reasoning [[Code will be released soon](https://github.com/Mr-Loevan/FAST)]
20. [arxiv 2504] Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension [[Code will be released soon](https://github.com/HKUST-LongGroup/Relation-R1)]
21. [arxiv 2504] Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation [[Code will be released soon](https://github.com/KAIST-Visual-AI-Group/APC-VLM)]
22. [arxiv 2504] Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict [[Code will be released soon](https://github.com/megagonlabs/Modality-Bias)]
23. [arxiv 2504] KeyMPs: One-Shot Vision-Language Guided Motion Generation by Sequencing DMPs for Occlusion-Rich Tasks
24. [arxiv 2503] Can Large Vision Language Models Read Maps Like a Human? [[Code will be released soon](https://github.com/taco-group/MapBench)]
25. [arxiv 2503] ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models [[Code will be released soon](https://github.com/mlvlab/ST-VLM)]
26. [arxiv 2503] Q-Insight: Understanding Image Quality via Visual Reinforcement Learning [[Code will be released soon](https://github.com/lwq20020127/Q-Insight)]
27. [arxiv 2503] VisualThinker-R1-Zero: R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model [[Code will be released soon](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)]
28. [arxiv 2503] Free-form language-based robotic reasoning and grasping [[Code will be released soon](https://tev-fbk.github.io/FreeGrasp/)]
29. [arxiv 2502] NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding with Explicit Logic Reasoning [[Code will be released soon](https://github.com/ControlNet/NAVER)]
30. [arxiv 2412] SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models [[Code will be released soon](https://arijitray.com/SAT/)]
31. [arxiv 2410] Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Spatial Reasoning
32. [arxiv 2406] RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model [[Code will be released soon](https://jity16.github.io/RoboGolf/)]
33. [arxiv 2402] PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs [[Code will be released soon](https://pivot-prompt.github.io/)]
34. [arxiv 2412] Video LLMs for Temporal Reasoning in Long Videos
35. [arxiv 2412] Improving Vision-Language-Action Models via Chain-of-Affordance [[Code will be released soon](https://chain-of-affordance.github.io/)]
36. [arxiv 2411] Learning from Feedback: Semantic Enhancement for Object SLAM Using Foundation Models [[Code will be released soon](http://jungseokhong.com/SEO-SLAM/)]
37. [arxiv 2312] Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models [[Code will be released soon](https://visual-program-distillation.github.io/)]
38. [ACM MM‚Äò24] RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector
39. [ICLR'25 workshop] Evaluating Multi-modal Language Models Through Concept Hacking



## Open-Source Projects (Repos without Paper)

### Training Framework
* [EasyR1 üíª](https://github.com/hiyouga/EasyR1)  ![EasyR1](https://img.shields.io/github/stars/hiyouga/EasyR1) 

### Embodied

* [OpenVLA üíª](https://github.com/openvla/openvla) ![EasyR1](https://img.shields.io/github/stars/openvla/openvla) 
* [PI Series üíª](https://github.com/Physical-Intelligence/openpi) ![PI Series](https://img.shields.io/github/stars/Physical-Intelligence/openpi)
* [MiniVLA üíª](https://github.com/Stanford-ILIAD/openvla-mini) ![MiniVLA](https://img.shields.io/github/stars/Stanford-ILIAD/openvla-mini)

### Image

* [R1-V üíª](https://github.com/Deep-Agent/R1-V)  ![R1-V](https://img.shields.io/github/stars/Deep-Agent/R1-V) 
* [Multimodal Open R1 üíª](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)  ![Multimodal Open R1](https://img.shields.io/github/stars/EvolvingLMMs-Lab/open-r1-multimodal)
* [MMR1 üíª](https://github.com/LengSicong/MMR1) ![LengSicong/MMR1](https://img.shields.io/github/stars/LengSicong/MMR1) 
* [R1-Multimodal-Journey üíª](https://github.com/FanqingM/R1-Multimodal-Journey) ![R1-Multimodal-Journey](https://img.shields.io/github/stars/FanqingM/R1-Multimodal-Journey) 
* [R1-Vision üíª](https://github.com/yuyq96/R1-Vision) ![R1-Vision](https://img.shields.io/github/stars/yuyq96/R1-Vision) 
* [Ocean-R1 üíª](https://github.com/VLM-RL/Ocean-R1)  ![Ocean-R1](https://img.shields.io/github/stars/VLM-RL/Ocean-R1) 
* [R1V-Free üíª](https://github.com/Exgc/R1V-Free)  ![Exgc/R1V-Free](https://img.shields.io/github/stars/Exgc/R1V-Free)
* [SeekWorld üíª](https://github.com/TheEighthDay/SeekWorld)  ![TheEighthDay/SeekWorld](https://img.shields.io/github/stars/TheEighthDay/SeekWorld) 
* [R1-Track üíª](https://github.com/Wangbiao2/R1-Track)  ![Wangbiao2/R1-Track](https://img.shields.io/github/stars/Wangbiao2/R1-Track)

### Video

* [Open R1 Video üíª](https://github.com/Wang-Xiaodong1899/Open-R1-Video) ![Open R1 Video](https://img.shields.io/github/stars/Wang-Xiaodong1899/Open-R1-Video) 
* [Temporal-R1 üíª](https://github.com/appletea233/Temporal-R1)  ![Temporal-R1](https://img.shields.io/github/stars/appletea233/Temporal-R1) 
* [Open-LLaVA-Video-R1 üíª](https://github.com/Hui-design/Open-LLaVA-Video-R1) ![Open-LLaVA-Video-R1](https://img.shields.io/github/stars/Hui-design/Open-LLaVA-Video-R1) 

### Agent 

* [VAGEN üíª](https://github.com/RAGEN-AI/VAGEN) ![VAGEN](https://img.shields.io/github/stars/RAGEN-AI/VAGEN)



## Acknowledgement

1. https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs
2. https://github.com/yaotingwangofficial/Awesome-MCoT
3. https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln
4. https://modelscope.cn/studios/AI-ModelScope/awesome-reasoning
5. https://github.com/Songwxuan/Embodied-AI-Paper-TopConf
6. https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List
7. https://github.com/zubair-irshad/Awesome-Robotics-3D
8. https://github.com/LightChen233/Awesome-Long-Chain-of-Thought-Reasoning
9. https://github.com/NishilBalar/Awesome-LVLM-Hallucination
10. https://github.com/zhengxuJosh/Awesome-RAG-Vision
11. https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning
12. https://github.com/BioRAILab/Neural-Brain-for-Embodied-Agents



## Citation

If you find this repository useful for your research and applications, please star us and consider citing:

```
@misc{Tang2025Awesome-spatial-visual-reasoning-MLLMs,
  title={Awesome-spatial-visual-reasoning-MLLMs},
  author={Jing Tang},
  year={2025},
  howpublished={\url{https://github.com/vaew/Awesome-spatial-visual-reasoning-MLLMs}},
  note={Github Repository},
}
```
